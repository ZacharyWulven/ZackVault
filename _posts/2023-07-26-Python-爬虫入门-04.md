---
layout: post
title: Python-爬虫入门-04
date: 2023-07-26 16:45:30.000000000 +09:00
categories: [Python, 爬虫]
tags: [Python, 爬虫]
---

# 22 Scrapy

## 简介
* Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的框架。可以应用在数据挖掘、信息处理或存储历史数据等程序中

## 安装

```bash
pip3 install scrapy
```

## 基本使用

### 1 创建项目

```bash
scrapy startproject 项目名称
```

### 2 创建爬虫文件
* 在项目名称/项目名称/spiders 文件夹下创建爬虫文件
* 创建爬虫文件命令

```bash
scrapy genspider 爬虫文件名称 要爬取的网页

#例
scrapy genspider mouzhan www.moumouzhan.com
```

> 一般情况下，不需要加 `http` 协议前缀，因为 `start_urls` 是根据 `allowed_domains` 改变的，`start_urls` 为 `allowed_domains` 前边拼上 `https://`，后边拼上 `/`
{: .prompt-info }

### 3 运行爬虫代码

```bash
scrapy crawl 爬虫名称

#例
scrapy crawl moumouzhan
```

> 运行后可能有反爬，可访问 `https://www.xxxx.com/robots.txt`，这叫君子协定，不让爬。可修改 `settings.py` 里边有 `ROBOTSTXT_OBEY = True`，将其注释即可。也就不遵守 `robots` 协议了。
{: .prompt-info }


## Scrapy 项目结构

```
项目名称
  |--项目名称
    |--spiders       文件夹，存储的是爬虫文件
    |  |--__init__.py
    |  |--自定义的爬虫文件，即核心功能文件，是最重要的
    |
    |--__init__.py
    |--items.py       定义数据结构的地方，爬取的数据包含哪些
    |--middlewares.py 中间件，未来要实现的代理机制
    |--pipelines.py   管道，用于处理下载的数据
    |--settings,py    配置文件，例如 robos 协议、UA 等在这里定义
```

## Scrapy 自定义脚本的 parse 方法的 response 参数的属性和方法

* 获取的响应的字符串

```python
response.text
```

* 获取的是二进制数据

```python
response.body
```

* 使用 xpath 语法解析 response 内容（常用）
* extract 提取 selector 对象的 data 属性值（常用）

```python
# span 返回的是 Selector 对象
span = response.xpath('//div[@id="filter"]//span')[0]

# extract 获取 Selector 对象的 data 属性的值
span_text = span.extract()
```

* 提取 selector 列表里的第一个数据（常用）

```python
response.extract_first()
```

### 例子

```python
import scrapy

class XXXSpider(scrapy.Spider):
    name = "xxx"
    allowed_domains = ["www.xxx.com"]
    # 如果请求的 url 是 html 为结尾的，则最后不能加 /
    start_urls = ["https://www.xxx/"]

    def parse(self, response):
        # pass

        # name_list 里是 Selector 对象
        name_list = response.xpath('//a[@class="xxx-info"]//h4[@class="xxx-name"]/text()')
        # print(name_list)
        print('name -----------------------------------------')
        # for name in name_list:
        #     print(name.extract())

        print('price -----------------------------------------')
        price_list = response.xpath('//a[@class="info"]//span//em/text()')
        # for price in price_list:
        #     print(price.extract())

        for i in range(len(name_list)):
            name = name_list[i].extract()
            price = price_list[i].extract()
            print(f'name={name}, price={price}')
```

> 如果请求的 url 是 html 为结尾的，则最后不能加 `/`
{: .prompt-info }


## Scrapy 工作原理
1. 引擎向 spiders（爬虫文件）要 url，引擎将要爬取的 url 给调度器
2. 调度器获取 url 生成请求对象放到队列里
3. 从队列里出列一个请求给引擎，引擎将这个请求给到下载器
4. 下载器发送请求去互联网获取数据
3. 下载器将请求的数据返回给引擎，引擎把数据再给到 spiders 进行解析，解析数据分两种情况
* 如果是 url 会在回到第一步
* 如果是数据，就交给管道进行下载了


![image](/assets/images/creep/scrapy.png)


## Scrapy Shell
* Scrapy终端是一个交互终端，供您在未启动spider的情况下尝试及调试您的爬取代码。 其本意是用来测试提取数据的代码，不过您可以将其作为正常的Python终端，在上面测试任何的Python代码。
该终端是用来测试XPath或CSS表达式，查看他们的工作方式及从爬取的网页中提取的数据。 在编写您的spider时，该终端提供了交互性测试您的表达式代码的功能，免去了每次修改后运行spider的麻烦。
一旦熟悉了Scrapy终端后，您会发现其在开发和调试spider时发挥的巨大作用。
如果您安装了 IPython ，Scrapy终端将使用 IPython (替代标准Python终端)。 IPython 终端与其他相比更为强大，提供智能的自动补全，高亮输出，及其他特性。

* 安装 ipython

```bash
pip3 install ipython
```

* 如何使用

```bash
scrapy shell 域名

# 然后可在终端进行调试，例如
In [4]: response.xpath('//input[@id="info"]')
```



## 单管道使用

### yield 
* 简单理解就是 return 一个返回值，提交给 pipeline。并且记住这个返回位置，下次迭代就从这个位置后（下一行）开始


### 流程

* Step 1：数据定义 `items.py`

```python
class ScrapyItem(scrapy.Item):
    # 你要下载的数据都有什么
    # 图片
    src = scrapy.Field()

    # 名字
    name = scrapy.Field()

    # 价格
    price = scrapy.Field()
```

* Step 2：spider 获取数据

```python
# 这里导入 ScrapyItem 虽然飘红但是其实没有问题
from scrapy_.items import ScrapyItem

    def parse(self, response):
        print('======================================')

        # pipelines 下载数据的

        # items 定义数据结构的
        # //ul[@id="xxx"]/li//img/@src
        # //ul[@id="xxx"]/li//img/@alt
        # //ul[@id="xxx"]/li//span[@class="price"]/text()
        # Tips: 所有的 Selector 对象，都可以再次调用 xpath 方法
        li_list = response.xpath('//ul[@id="xxx"]/li')

        for li in li_list:
            # 在 li Selector 基础上再次调用 xpath 获取当前标签下的 img
            name = li.xpath('.//img/@alt').extract_first()
            price = li.xpath('.//span[@class="price"]/text()').extract_first()

            # 图片的坑
            # 1 这里注意图片有反爬懒加载，属性应该是 data-original，而不是 src
            # 2 第一张图片和其他图片的标签属性是不一样的
            # 即第一张图片的 src 是可以使用的
            # 其他图片的地址是 data-original
            src = li.xpath('.//img/@data-original').extract_first()
            if src is None:
                src = li.xpath('.//img/@src').extract_first()

            print(src, name, price)
            data = ScrapyItem(src=src, name=name, price=price)

            # 将对象给管道：获取一个 data 就将其交给 pipelines
            yield data
```


* Step 3：管道使用
1. 如果想使用管道的话，就必须在 `settings.py` 里开启管道，将其放开注释
2. 管道可以有很多个，管道是有优先级的，值的范围是 `1~1000`，值越小优先级越高

```python
ITEM_PIPELINES = {
   "scrapy.pipelines.ScrapyPipeline": 300,
}
```


* Step 4：spider 代码使用 yield 提交给管道 `pipelines.py`

```python
class Scrapy_Pipeline:
    # 在爬虫开始执行前 就执行的方法
    def open_spider(self, spider):
        print('++++++++++++++++++++++++')
        self.fp = open('data.json', 'w', encoding='utf-8')


    # item 就是 yield 后边返回的对象
    def process_item(self, item, spider):
        # 1 write 方法必须接收字符串，而不能是其他的对象
        # 解决：需要强转为 str(item)
        # 2 w 模式会每一个对象都打开一次文件，覆盖之前的内容然后关闭，
        # 解决：所以要用 a 模式（追加）
        # 这种写法不推荐，因为会频繁操作文件
        # with open('data.json', 'a', encoding='utf-8') as fp:
        #     fp.write(str(item))

        # 优化为这样
        self.fp.write(str(item))
        return item


    # 在爬虫文件执行结束后 执行的方法
    def close_spider(self, spider):
        print('--------------------------')
        self.fp.close()
```




## 多条管道下载

* Step 1：在 `pipelines.py` 定义添加新的管道类
* Step 2：在 `setting.py` 中开启管道: `"scrapy.pipelines.ScrapyDownloadPipeline": 301`，由于下载比较慢设置优先级低一些



```python
# 多条管道同时开启
# 1 定义新的管道类
# 2 在 setting.py 中开启管道: "scrapy.pipelines.ScrapyDownloadPipeline": 301,


import urllib.request
class ScrapyDownloadPipeline:

    def process_item(self, item, spider):
        url = 'http:' + item.get('src')
        filename = './images/' + item.get('name') + '.jpg'
        urllib.request.urlretrieve(url=url, filename=filename)

        return item
```



## 多页数据下载

* 多页下载必须调整 `allowed_domains` 的范围，一般只写域名


```python
import scrapy

# 这里导入 虽然飘红但是没有问题
from scrapy.items import ScrapyItem

class Spider(scrapy.Spider):
    name = ""
    # 如果是多页下载，必须调整 allowed_domains 的范围，一般只写域名
    allowed_domains = ["www.xxx.com"]
    start_urls = ["https://"]

    base_url = 'http://www.com/'
    page = 1
    def parse(self, response):
        print('======================================')

        # pipelines 下载数据的

        # items 定义数据结构的
        # //ul[@id="xxx"]/li//img/@src
        # //ul[@id="xxx"]/li//img/@alt
        # //ul[@id="xxx"]/li//span[@class="price"]/text()
        # 所有的 Selector 对象，都可以再次调用 xpath 方法
        li_list = response.xpath('//ul[@id="xxx"]/li')

        for li in li_list:
            # 在 li Selector 基础上再次调用 xpath 获取当前标签下的 img
            name = li.xpath('.//img/@alt').extract_first()
            price = li.xpath('.//span[@class="price"]/text()').extract_first()

            # 图片的坑
            # 1 这里注意图片有反爬懒加载，属性应该是 data-original，而不是 src
            # 2 第一张图片和其他图片的标签属性是不一样的
            # 即第一张图片的 src 是可以使用的
            # 其他图片的地址是 data-original
            src = li.xpath('.//img/@data-original').extract_first()
            if src is None:
                src = li.xpath('.//img/@src').extract_first()

            print(src, name, price)
            book = ScrapyItem(src=src, name=name, price=price)

            # 将对象给管道：获取一个 book 就将其交给 pipelines
            yield book


        if self.page < 3:
            self.page = self.page + 1
            url = self.base_url + str(self.page) + '.html'

            # 如何调用 parse 方法呢?
            # scrapy.Request 就是 scrapy 的 get 请求
            # 参数 url 就是请求地址
            # 参数 callback 就是要执行的函数，注意不能加 '()'
            yield scrapy.Request(url=url, callback=self.parse)
```


## 多请求参数传递

* 使用 `meta` 参数进行参数传递

```python
import scrapy
from scrapy.items import ScrapyItem

class Spider(scrapy.Spider):
    name = ""
    allowed_domains = ["www.xxx."]
    start_urls = ["https://www.xxx."]

    def parse(self, response):
        # pass
        # 要第一页的名字，第二页的图片

        page_base_url = 'https://www.xxx.'

        # td[2] 为第二个 td 标签
        a_list = response.xpath('//div[@class="co_content8"]//td[2]//a[@class="ulink"]')

        for a in a_list:
            # print(a)
            name = a.xpath('./text()').extract_first()
            href = a.xpath('./@href').extract_first()

            # 第二页地址
            href_url = page_base_url + href
            # print(f'name is {name}, url = {href_url}')

            # 对第二页 url 发起请求
            # meta 用于将 name 传参给第二个函数
            yield scrapy.Request(url=href_url, callback=self.parse_href, meta={'name': name})


    def parse_href(self, response):
        # 如果拿不到数据，首先要检查 xpath 语法是否正确
        # span 可能有时识别不了
        src = response.xpath('//div[@id="Zoom"]//img/@src').extract_first()
        print(f'parse_href={src}')

        # 接收到请求 meta 那个参数的值
        name = response.meta['name']
        print(f'name is {name}')

        item = ScrapyItem(src=src, name=name)

        yield item
        
```

## CrawlSpider

* 它继承自 scrapy.Spider
* CrawlSpider 可以定义规则，在解析 html 内容时，可以根据链接规则提取出指定的链接，然后再向这些链接发送请求。所以如果有跟进链接的需求，例如爬取链接后，根据链接再次爬取
使用 CrawlSpider 是非常好用的

### 提取链接

* 链接提取器语法

```python
scrapy.linkextractors.LinkExtractor(
allow = (),           # 正则表达式，提取符合正则的链接
deny = (),            # 正则表达式，不提取符合正则的链接（不用）
allow_domains = (),   # 允许的域名（不用）
deny_domains = (),    # 不允许的域名（不用）
restrict_xpaths = (), # xpath，提取符合 xpath 的链接
restrict_css = (),    # 提取符合选择器规则的链接
```

* 在 scrapy shell 中使用 CrawlSpider

```bash
$ scrapy shell www.xxx.com
$ from scrapy.linkextractors import LinkExtractor


# 使用正则表达式提取：\d+ 匹配一个或多个数字
# \. 点前加 \ 是转义，因为有时候不生效
$ link = LinkExtractor(allow=r'/xxx/xxx_\d+\.html')
$ link.extract_links(response)


# 使用 xpath 进行提取
$ link = LinkExtractor(restrict_xpaths=r'//div[@class="pages"]/a/@href'
$ link.extract_links(response)
```

### CrawlSpider 案例

* 1 创建代码文件

```bash
$ scrapy startproject [项目名称]
$ cd ./[项目名称]/[项目名称]/spider
$ scrapy genspider -t crawl [脚本名称] [域名]
```

* 2 MySQL 及 Pipeline 相关配置

编辑 `settings.py`

```python

# DB 所在的 ip
DB_HOST = 'localhost'
# DB 端口号
DB_PORT = 3306
DB_USER = 'root'
DB_PASSWORD = '1234'
DB_NAME = 'spider_01'
# utf-8 的 - 不能写
DB_CHARSET = 'utf8'


# Configure item pipelines
# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
   "scrapy.pipelines.ScrapyPipeline": 300,
   "scrapy.pipelines.MySQLPipeline": 301,
}
```

* 3 安装 MySQL

```bash
# 安装
$ brew install mysql

# 设置密码等配置
$ mysql_secure_installation

# 停止服务
$ mysql.server stop

# 开启服务
$ mysql.server start

# 通过用户名：root，密码：1234
$ mysql -uroot -p1234
```

* 4 创建数据库

```shell
# 创建数据库
mysql> create database [数据库名称] charset=utf8;
Query OK, 1 row affected, 1 warning (0.01 sec)

# 使用创建的数据库
mysql> use [数据库名称];
Database changed

# 创建表
mysql> create table [表名称](
    -> id int primary key auto_increment,
    -> name varchar(128), # 128 为长度
    -> src varchar(128));
Query OK, 0 rows affected (0.01 sec)

# 查询表
mysql> select * from [表名];
Empty set (0.00 sec)
```

* 5 安装 pymysql

```bash
$ cd /Library/Frameworks/Python.framework/Versions/3.9/bin
$ pip3 install pymysql
```
* 6 爬取代码

```python
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from scrapy.items import ScrapyItem

class XXXXSpider(CrawlSpider):
    name = ""

    # 要特别注意 allowed_domains 字段，允许的域名
    allowed_domains = ["www.xxxx.com"]

    # 注意：下边的正则无法匹配第一页 url，因为第一页 url 没有 _
    # start_urls = ["https://www."]
    # 所以修改为下边这样就可以了
    start_urls = ["https://ww"]

    rules = (
        Rule(LinkExtractor(allow=r'/xxxx_\d+\.html'),
                           callback="parse_item",
             # follow 表示继续跟进改为 True 就可以下载所有数据，否则只能是当前的那些页的数据
                           follow=True),
    )

    def parse_item(self, response):
        item = {}

        # //div[@class="book-info"]//a/img/@alt
        # //div[@class="book-info"]//a/img/@data-original
        a_list = response.xpath('//div[@class="book-info"]//a/img')

        for a in a_list:
            name = a.xpath('./@alt').extract_first()
            src = a.xpath('./@data-original').extract_first()
            # pic = a.xpath('./@src').extract_first()
            print(f'name={name}, pic={src}')

            item = ScrapyItem(name=name, src=src)
            yield item
```

* 7 自定义管道代码

```python
from scrapy.utils.project import get_project_settings
import pymysql

class MySQLPipeline:

    def open_spider(self, spider):
        settings = get_project_settings()
        self.host = settings['DB_HOST']
        self.port = settings['DB_PORT']
        self.user = settings['DB_USER']
        self.pwd = settings['DB_PASSWORD']
        self.name = settings['DB_NAME']
        self.charset = settings['DB_CHARSET']

        self.connect()

    def connect(self):
        self.conn = pymysql.connect(
            host=self.host,
            port=self.port,
            user=self.user,
            password=self.pwd,
            db=self.name,
            charset=self.charset
        )

        self.cursor = self.conn.cursor()


    def close_spider(self, spider):
        self.cursor.close()
        self.conn.close()


    def process_item(self, item, spider):
        sql = 'insert into xxx(name, src) values("{}", "{}")'.format(item['name'], item['src'])
        # 执行 sql 语句
        self.cursor.execute(sql)
        # 提交
        self.conn.commit()

        return item
```


> 坑：如果除第一页 url 是这样， `https://www.xxx.com/xxxx_2.html`，那么正则匹配 `r'/xxx/xxx_\d+\.html'` 会无法匹配到第一页，所以 `start_urls` 里的起始页也要改成对应格式
{: .prompt-info }


## Scrapy 日志信息及日志等级

### 日志等级
1. `CRITICAL` 严重错误
2. `ERROR` 一般错误
3. `WARNING` 警告
4. `INFO` 一般
5. `DEBUG` 调试信息

> 默认等级是 `DEBUG`，只要出现了 `DEBUG` 或高于 `DEBUG` 等级的日志就会打印
{: .prompt-info }


### 修改 `settings.py` 文件

```python
# 指定 log 等级，但一般不会调整等级，这里只是知道一下
LOG_LEVEL="WARNING"

# 指定 log 输出文件，后缀必须是 `.log`
LOG_FILE="logdemo.log"
```

## Scrapy Post Request

* `post` 请求不使用 `start_urls` 和 `parse` 方法
* 使用 `yield scrapy.FormRequest` 发送 `post` 请求

```python

import scrapy
import json

class PostdemoSpider(scrapy.Spider):
    name = "postdemo"
    allowed_domains = [""]

    # 注意：post 请求，不用 start_urls 和 parse 方法
    # start_urls = ["https://"]
    # def parse_second(self, response):
    #       pass


    # post 请求必须使用 start_requests，这是 scrapy 框架提供的
    def start_requests(self):
        url = 'https:'

        data = {
            '': 'a'
        }

        # 发送 post 请求
        yield scrapy.FormRequest(url=url, formdata=data, callback=self.parse_second)


    # 自定义一个 callback 方法
    def parse_second(self, response):
        print('====================================')
        content = response.text
        print(content)
        print('++++++++++++++++++++++++++++++++++++++')
        obj = json.loads(content)
        print(obj)

        with open('post.json', 'w', encoding='UTF-8') as fp:
            fp.write(str(obj))
```


